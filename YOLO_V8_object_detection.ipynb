{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04969552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (8.0.205)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.22.2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (1.26.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (1.11.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.16.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.13.0)\n",
      "Requirement already satisfied: psutil in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
      "Requirement already satisfied: filelock in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.8.0)\n",
      "Requirement already satisfied: sympy in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: colorama in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f4226da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb7c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import models\n",
    "# import torchvision.transforms as T\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019d0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a644e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import cv2\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31bd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c54ac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\reva_hack\\\\model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce080a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 274\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    276\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    255\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    260\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    261\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    262\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    263\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "#model = torch.load('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbce8593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\reva_hack\\model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dc4d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"D:\\reva_hack\\model\\best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fcf538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_=YOLO(\"best.pt\")\n",
    "#inference = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb094279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337c6233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 2 plastics, 261.7ms\n",
      "Speed: 4.7ms preprocess, 261.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 328.5ms\n",
      "Speed: 3.0ms preprocess, 328.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 304.8ms\n",
      "Speed: 0.0ms preprocess, 304.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 275.2ms\n",
      "Speed: 0.0ms preprocess, 275.2ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 309.9ms\n",
      "Speed: 3.0ms preprocess, 309.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 281.0ms\n",
      "Speed: 2.0ms preprocess, 281.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 295.4ms\n",
      "Speed: 2.0ms preprocess, 295.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 302.3ms\n",
      "Speed: 3.8ms preprocess, 302.3ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 274.3ms\n",
      "Speed: 0.0ms preprocess, 274.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 314.2ms\n",
      "Speed: 0.0ms preprocess, 314.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 292.8ms\n",
      "Speed: 2.0ms preprocess, 292.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 339.3ms\n",
      "Speed: 4.0ms preprocess, 339.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 313.0ms\n",
      "Speed: 5.4ms preprocess, 313.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 272.2ms\n",
      "Speed: 3.0ms preprocess, 272.2ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 302.4ms\n",
      "Speed: 4.2ms preprocess, 302.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 264.5ms\n",
      "Speed: 2.1ms preprocess, 264.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 310.5ms\n",
      "Speed: 2.1ms preprocess, 310.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 305.4ms\n",
      "Speed: 0.0ms preprocess, 305.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 283.5ms\n",
      "Speed: 0.0ms preprocess, 283.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 290.5ms\n",
      "Speed: 4.0ms preprocess, 290.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 261.2ms\n",
      "Speed: 3.0ms preprocess, 261.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 269.0ms\n",
      "Speed: 4.5ms preprocess, 269.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 290.4ms\n",
      "Speed: 5.0ms preprocess, 290.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 285.6ms\n",
      "Speed: 2.0ms preprocess, 285.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 274.9ms\n",
      "Speed: 2.1ms preprocess, 274.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 278.7ms\n",
      "Speed: 4.0ms preprocess, 278.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 282.9ms\n",
      "Speed: 3.0ms preprocess, 282.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 295.4ms\n",
      "Speed: 0.0ms preprocess, 295.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 302.3ms\n",
      "Speed: 3.0ms preprocess, 302.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 264.5ms\n",
      "Speed: 2.1ms preprocess, 264.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 277.3ms\n",
      "Speed: 5.3ms preprocess, 277.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 301.7ms\n",
      "Speed: 3.6ms preprocess, 301.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 276.0ms\n",
      "Speed: 0.0ms preprocess, 276.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 1 plastic, 268.2ms\n",
      "Speed: 4.0ms preprocess, 268.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 263.5ms\n",
      "Speed: 0.0ms preprocess, 263.5ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 304.3ms\n",
      "Speed: 2.2ms preprocess, 304.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 304.1ms\n",
      "Speed: 3.1ms preprocess, 304.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 276.6ms\n",
      "Speed: 2.5ms preprocess, 276.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 306.0ms\n",
      "Speed: 3.0ms preprocess, 306.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 272.9ms\n",
      "Speed: 2.5ms preprocess, 272.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 295.9ms\n",
      "Speed: 2.6ms preprocess, 295.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 271.3ms\n",
      "Speed: 2.0ms preprocess, 271.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 244.7ms\n",
      "Speed: 2.5ms preprocess, 244.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 279.3ms\n",
      "Speed: 4.1ms preprocess, 279.3ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 314.6ms\n",
      "Speed: 0.0ms preprocess, 314.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 304.0ms\n",
      "Speed: 2.5ms preprocess, 304.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 2 plastics, 281.7ms\n",
      "Speed: 2.7ms preprocess, 281.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 3 drys, 302.7ms\n",
      "Speed: 2.0ms preprocess, 302.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 320.3ms\n",
      "Speed: 3.0ms preprocess, 320.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 335.1ms\n",
      "Speed: 2.0ms preprocess, 335.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 314.7ms\n",
      "Speed: 2.6ms preprocess, 314.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 315.9ms\n",
      "Speed: 2.5ms preprocess, 315.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 314.0ms\n",
      "Speed: 0.0ms preprocess, 314.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 314.2ms\n",
      "Speed: 2.0ms preprocess, 314.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 231.7ms\n",
      "Speed: 2.1ms preprocess, 231.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 264.2ms\n",
      "Speed: 0.0ms preprocess, 264.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 303.5ms\n",
      "Speed: 2.0ms preprocess, 303.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 304.3ms\n",
      "Speed: 0.0ms preprocess, 304.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 294.1ms\n",
      "Speed: 1.0ms preprocess, 294.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 298.5ms\n",
      "Speed: 3.9ms preprocess, 298.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 303.5ms\n",
      "Speed: 2.0ms preprocess, 303.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 314.6ms\n",
      "Speed: 2.0ms preprocess, 314.6ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 302.6ms\n",
      "Speed: 2.0ms preprocess, 302.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 2 plastics, 292.8ms\n",
      "Speed: 3.0ms preprocess, 292.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 313.3ms\n",
      "Speed: 2.0ms preprocess, 313.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 253.6ms\n",
      "Speed: 2.2ms preprocess, 253.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 286.3ms\n",
      "Speed: 3.0ms preprocess, 286.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 284.7ms\n",
      "Speed: 3.0ms preprocess, 284.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 295.5ms\n",
      "Speed: 4.4ms preprocess, 295.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 272.4ms\n",
      "Speed: 0.0ms preprocess, 272.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 294.3ms\n",
      "Speed: 5.0ms preprocess, 294.3ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 291.9ms\n",
      "Speed: 3.6ms preprocess, 291.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 313.3ms\n",
      "Speed: 2.0ms preprocess, 313.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 305.7ms\n",
      "Speed: 3.5ms preprocess, 305.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 313.6ms\n",
      "Speed: 2.1ms preprocess, 313.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 281.2ms\n",
      "Speed: 0.0ms preprocess, 281.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 292.5ms\n",
      "Speed: 2.2ms preprocess, 292.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 285.0ms\n",
      "Speed: 1.8ms preprocess, 285.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 1 plastic, 292.3ms\n",
      "Speed: 4.3ms preprocess, 292.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 284.3ms\n",
      "Speed: 0.0ms preprocess, 284.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 1 plastic, 292.5ms\n",
      "Speed: 1.2ms preprocess, 292.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 283.3ms\n",
      "Speed: 2.3ms preprocess, 283.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 323.2ms\n",
      "Speed: 3.7ms preprocess, 323.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 314.5ms\n",
      "Speed: 0.0ms preprocess, 314.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 295.8ms\n",
      "Speed: 0.0ms preprocess, 295.8ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 293.7ms\n",
      "Speed: 3.0ms preprocess, 293.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 1 plastic, 320.3ms\n",
      "Speed: 1.7ms preprocess, 320.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 301.4ms\n",
      "Speed: 2.0ms preprocess, 301.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 294.6ms\n",
      "Speed: 3.6ms preprocess, 294.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 295.6ms\n",
      "Speed: 3.5ms preprocess, 295.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 1 plastic, 315.1ms\n",
      "Speed: 0.0ms preprocess, 315.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 275.4ms\n",
      "Speed: 2.1ms preprocess, 275.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 312.8ms\n",
      "Speed: 3.0ms preprocess, 312.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 303.7ms\n",
      "Speed: 3.0ms preprocess, 303.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 304.0ms\n",
      "Speed: 0.0ms preprocess, 304.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 303.4ms\n",
      "Speed: 2.2ms preprocess, 303.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 drys, 292.7ms\n",
      "Speed: 5.4ms preprocess, 292.7ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "#from yolov8 import YOLOv8\n",
    "\n",
    "# Load the webcam stream\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Define the inference callback\n",
    "def inference(frame):\n",
    "    # Convert the frame to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect objects in the frame\n",
    "    results = inference_.predict(frame, show=True)\n",
    "\n",
    "    # Draw bounding boxes around the detected objects\n",
    "#     for detection in results:\n",
    "#         cv2.rectangle(frame, (detection[0:2]), (detection[2:4]), (255, 0, 0), 2)\n",
    "#         cv2.putText(frame, detection[5], detection[0:2], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    \n",
    "\n",
    "    # Return the frame\n",
    "    return frame\n",
    "\n",
    "# Start the inference loop\n",
    "while True:\n",
    "    # Get the next frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame is not empty, run the inference callback\n",
    "    if ret:\n",
    "        frame = inference(frame)\n",
    "\n",
    "    # Show the frame\n",
    "    #cv2.imshow(\"YOLOv8 Webcam Inference\", frame)\n",
    "\n",
    "    # Wait for a keypress\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Close the webcam\n",
    "cap.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687e9fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 plastic, 1 wet, 261.0ms\n",
      "Speed: 0.0ms preprocess, 261.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 282.9ms\n",
      "Speed: 2.0ms preprocess, 282.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 222.0ms\n",
      "Speed: 0.0ms preprocess, 222.0ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 2 wets, 226.0ms\n",
      "Speed: 2.0ms preprocess, 226.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 240.2ms\n",
      "Speed: 0.0ms preprocess, 240.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 wets, 243.4ms\n",
      "Speed: 4.7ms preprocess, 243.4ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 300.2ms\n",
      "Speed: 0.0ms preprocess, 300.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 282.9ms\n",
      "Speed: 0.0ms preprocess, 282.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 265.5ms\n",
      "Speed: 2.0ms preprocess, 265.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 2 wets, 313.3ms\n",
      "Speed: 4.6ms preprocess, 313.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 305.9ms\n",
      "Speed: 4.5ms preprocess, 305.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 325.0ms\n",
      "Speed: 0.0ms preprocess, 325.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 309.4ms\n",
      "Speed: 3.4ms preprocess, 309.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 296.7ms\n",
      "Speed: 2.2ms preprocess, 296.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 291.4ms\n",
      "Speed: 3.7ms preprocess, 291.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 296.3ms\n",
      "Speed: 2.0ms preprocess, 296.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 323.3ms\n",
      "Speed: 2.2ms preprocess, 323.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 282.7ms\n",
      "Speed: 0.0ms preprocess, 282.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 232.8ms\n",
      "Speed: 0.0ms preprocess, 232.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 295.3ms\n",
      "Speed: 0.0ms preprocess, 295.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 wets, 302.1ms\n",
      "Speed: 2.7ms preprocess, 302.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 wets, 292.9ms\n",
      "Speed: 0.0ms preprocess, 292.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 251.1ms\n",
      "Speed: 0.0ms preprocess, 251.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 2 wets, 300.8ms\n",
      "Speed: 1.0ms preprocess, 300.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 353.3ms\n",
      "Speed: 0.5ms preprocess, 353.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 349.3ms\n",
      "Speed: 0.0ms preprocess, 349.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 322.3ms\n",
      "Speed: 0.0ms preprocess, 322.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 328.1ms\n",
      "Speed: 2.6ms preprocess, 328.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 265.3ms\n",
      "Speed: 3.1ms preprocess, 265.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 262.5ms\n",
      "Speed: 0.0ms preprocess, 262.5ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 286.0ms\n",
      "Speed: 1.0ms preprocess, 286.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 269.5ms\n",
      "Speed: 0.0ms preprocess, 269.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 295.0ms\n",
      "Speed: 0.0ms preprocess, 295.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 277.2ms\n",
      "Speed: 2.0ms preprocess, 277.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 244.9ms\n",
      "Speed: 4.1ms preprocess, 244.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 257.6ms\n",
      "Speed: 2.1ms preprocess, 257.6ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 267.3ms\n",
      "Speed: 4.2ms preprocess, 267.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 313.1ms\n",
      "Speed: 2.0ms preprocess, 313.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 242.2ms\n",
      "Speed: 0.0ms preprocess, 242.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 311.1ms\n",
      "Speed: 2.3ms preprocess, 311.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 254.9ms\n",
      "Speed: 2.0ms preprocess, 254.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 317.6ms\n",
      "Speed: 2.0ms preprocess, 317.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 345.8ms\n",
      "Speed: 0.0ms preprocess, 345.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 292.8ms\n",
      "Speed: 2.0ms preprocess, 292.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 266.3ms\n",
      "Speed: 1.0ms preprocess, 266.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 266.4ms\n",
      "Speed: 0.0ms preprocess, 266.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 240.0ms\n",
      "Speed: 0.0ms preprocess, 240.0ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 293.2ms\n",
      "Speed: 2.0ms preprocess, 293.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 281.0ms\n",
      "Speed: 0.0ms preprocess, 281.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 301.2ms\n",
      "Speed: 0.4ms preprocess, 301.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 255.8ms\n",
      "Speed: 2.0ms preprocess, 255.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 287.7ms\n",
      "Speed: 3.8ms preprocess, 287.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 280.1ms\n",
      "Speed: 0.0ms preprocess, 280.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 289.6ms\n",
      "Speed: 2.0ms preprocess, 289.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 301.3ms\n",
      "Speed: 2.0ms preprocess, 301.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 292.8ms\n",
      "Speed: 0.0ms preprocess, 292.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 295.1ms\n",
      "Speed: 1.5ms preprocess, 295.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 262.7ms\n",
      "Speed: 0.0ms preprocess, 262.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 265.8ms\n",
      "Speed: 0.0ms preprocess, 265.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 305.1ms\n",
      "Speed: 0.0ms preprocess, 305.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 279.4ms\n",
      "Speed: 0.0ms preprocess, 279.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 294.7ms\n",
      "Speed: 2.4ms preprocess, 294.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 wet, 267.8ms\n",
      "Speed: 1.1ms preprocess, 267.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 267.9ms\n",
      "Speed: 3.0ms preprocess, 267.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 273.4ms\n",
      "Speed: 0.0ms preprocess, 273.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 248.4ms\n",
      "Speed: 2.0ms preprocess, 248.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 (no detections), 283.4ms\n",
      "Speed: 0.0ms preprocess, 283.4ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 272.1ms\n",
      "Speed: 0.0ms preprocess, 272.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 289.5ms\n",
      "Speed: 0.0ms preprocess, 289.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 262.9ms\n",
      "Speed: 0.9ms preprocess, 262.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 243.5ms\n",
      "Speed: 0.0ms preprocess, 243.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 270.3ms\n",
      "Speed: 0.0ms preprocess, 270.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 275.9ms\n",
      "Speed: 3.5ms preprocess, 275.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 267.1ms\n",
      "Speed: 0.0ms preprocess, 267.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 270.6ms\n",
      "Speed: 2.0ms preprocess, 270.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 291.0ms\n",
      "Speed: 0.0ms preprocess, 291.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 232.1ms\n",
      "Speed: 3.6ms preprocess, 232.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 260.2ms\n",
      "Speed: 0.0ms preprocess, 260.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 273.0ms\n",
      "Speed: 0.0ms preprocess, 273.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 253.0ms\n",
      "Speed: 0.0ms preprocess, 253.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 283.1ms\n",
      "Speed: 0.0ms preprocess, 283.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 340.5ms\n",
      "Speed: 0.0ms preprocess, 340.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 353.6ms\n",
      "Speed: 0.0ms preprocess, 353.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 307.8ms\n",
      "Speed: 0.5ms preprocess, 307.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 317.1ms\n",
      "Speed: 3.1ms preprocess, 317.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 320.0ms\n",
      "Speed: 0.0ms preprocess, 320.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 337.3ms\n",
      "Speed: 0.0ms preprocess, 337.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 wets, 348.0ms\n",
      "Speed: 0.0ms preprocess, 348.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 315.2ms\n",
      "Speed: 0.0ms preprocess, 315.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 345.4ms\n",
      "Speed: 2.0ms preprocess, 345.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 329.6ms\n",
      "Speed: 4.1ms preprocess, 329.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 366.3ms\n",
      "Speed: 0.0ms preprocess, 366.3ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 314.6ms\n",
      "Speed: 0.0ms preprocess, 314.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 282.9ms\n",
      "Speed: 2.1ms preprocess, 282.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 281.1ms\n",
      "Speed: 1.2ms preprocess, 281.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 256.3ms\n",
      "Speed: 1.0ms preprocess, 256.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 280.8ms\n",
      "Speed: 2.1ms preprocess, 280.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 264.5ms\n",
      "Speed: 2.1ms preprocess, 264.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 225.2ms\n",
      "Speed: 1.0ms preprocess, 225.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 270.5ms\n",
      "Speed: 0.0ms preprocess, 270.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 258.9ms\n",
      "Speed: 2.0ms preprocess, 258.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 301.5ms\n",
      "Speed: 0.0ms preprocess, 301.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 305.8ms\n",
      "Speed: 2.0ms preprocess, 305.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 286.3ms\n",
      "Speed: 0.0ms preprocess, 286.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 310.4ms\n",
      "Speed: 0.0ms preprocess, 310.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 277.1ms\n",
      "Speed: 0.0ms preprocess, 277.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 334.8ms\n",
      "Speed: 2.0ms preprocess, 334.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 348.5ms\n",
      "Speed: 0.0ms preprocess, 348.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 343.0ms\n",
      "Speed: 0.0ms preprocess, 343.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 341.0ms\n",
      "Speed: 1.7ms preprocess, 341.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 335.1ms\n",
      "Speed: 3.6ms preprocess, 335.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 342.6ms\n",
      "Speed: 0.0ms preprocess, 342.6ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 315.8ms\n",
      "Speed: 1.5ms preprocess, 315.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 291.6ms\n",
      "Speed: 2.0ms preprocess, 291.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 258.6ms\n",
      "Speed: 0.0ms preprocess, 258.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 237.7ms\n",
      "Speed: 2.1ms preprocess, 237.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 307.6ms\n",
      "Speed: 0.0ms preprocess, 307.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 257.5ms\n",
      "Speed: 0.0ms preprocess, 257.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 263.1ms\n",
      "Speed: 0.0ms preprocess, 263.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 262.0ms\n",
      "Speed: 0.0ms preprocess, 262.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 323.7ms\n",
      "Speed: 0.0ms preprocess, 323.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 269.4ms\n",
      "Speed: 0.0ms preprocess, 269.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 312.1ms\n",
      "Speed: 0.0ms preprocess, 312.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 238.9ms\n",
      "Speed: 2.0ms preprocess, 238.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 260.5ms\n",
      "Speed: 0.0ms preprocess, 260.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 wet, 231.3ms\n",
      "Speed: 0.0ms preprocess, 231.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 230.7ms\n",
      "Speed: 2.0ms preprocess, 230.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 261.1ms\n",
      "Speed: 2.0ms preprocess, 261.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 274.6ms\n",
      "Speed: 2.0ms preprocess, 274.6ms inference, 8.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 260.1ms\n",
      "Speed: 0.0ms preprocess, 260.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 292.9ms\n",
      "Speed: 0.0ms preprocess, 292.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 219.9ms\n",
      "Speed: 1.0ms preprocess, 219.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 283.8ms\n",
      "Speed: 0.0ms preprocess, 283.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 266.4ms\n",
      "Speed: 1.0ms preprocess, 266.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 279.2ms\n",
      "Speed: 0.0ms preprocess, 279.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 283.3ms\n",
      "Speed: 2.0ms preprocess, 283.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 366.5ms\n",
      "Speed: 0.0ms preprocess, 366.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 283.9ms\n",
      "Speed: 2.1ms preprocess, 283.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 307.0ms\n",
      "Speed: 0.0ms preprocess, 307.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 311.0ms\n",
      "Speed: 1.0ms preprocess, 311.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 323.0ms\n",
      "Speed: 0.0ms preprocess, 323.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 308.3ms\n",
      "Speed: 0.0ms preprocess, 308.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 285.0ms\n",
      "Speed: 2.5ms preprocess, 285.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 309.7ms\n",
      "Speed: 2.0ms preprocess, 309.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 272.3ms\n",
      "Speed: 1.5ms preprocess, 272.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 272.2ms\n",
      "Speed: 2.0ms preprocess, 272.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 314.3ms\n",
      "Speed: 0.0ms preprocess, 314.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 332.6ms\n",
      "Speed: 0.0ms preprocess, 332.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 271.3ms\n",
      "Speed: 2.0ms preprocess, 271.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 308.1ms\n",
      "Speed: 0.0ms preprocess, 308.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 281.8ms\n",
      "Speed: 0.0ms preprocess, 281.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 316.9ms\n",
      "Speed: 1.0ms preprocess, 316.9ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 1236.0ms\n",
      "Speed: 12.6ms preprocess, 1236.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 786.7ms\n",
      "Speed: 8.4ms preprocess, 786.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 254.6ms\n",
      "Speed: 2.5ms preprocess, 254.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 300.0ms\n",
      "Speed: 0.0ms preprocess, 300.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 333.1ms\n",
      "Speed: 1.5ms preprocess, 333.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 345.4ms\n",
      "Speed: 3.6ms preprocess, 345.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 348.7ms\n",
      "Speed: 0.0ms preprocess, 348.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 348.4ms\n",
      "Speed: 3.0ms preprocess, 348.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 342.4ms\n",
      "Speed: 2.0ms preprocess, 342.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 1 wet, 372.5ms\n",
      "Speed: 0.0ms preprocess, 372.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 wet, 365.4ms\n",
      "Speed: 0.0ms preprocess, 365.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Make detections \u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43minference_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#cv2.imshow('YOLO', np.squeeze(results.render()))\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\engine\\model.py:101\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\engine\\model.py:242\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\engine\\predictor.py:196\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\engine\\predictor.py:259\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 259\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\engine\\predictor.py:135\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    133\u001b[0m visualize \u001b[38;5;241m=\u001b[39m increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem,\n\u001b[0;32m    134\u001b[0m                            mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:347\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize)\u001b[0m\n\u001b[0;32m    344\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize) \u001b[38;5;28;01mif\u001b[39;00m augment \u001b[38;5;129;01mor\u001b[39;00m visualize \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\nn\\tasks.py:42\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\nn\\tasks.py:59\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\nn\\tasks.py:79\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m---> 79\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m     80\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:203\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 203\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:203\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    202\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 203\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:311\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    310\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:40\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     # Make detections \n",
    "#     results = inference_(frame)\n",
    "    \n",
    "#     #cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "#     if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e63e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (8.0.205)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.22.2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (1.26.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (1.11.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.16.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.13.0)\n",
      "Requirement already satisfied: psutil in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
      "Requirement already satisfied: filelock in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.8.0)\n",
      "Requirement already satisfied: sympy in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: colorama in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b066c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics.yolo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv8\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectionPredictor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics.yolo'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe1b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_v8",
   "language": "python",
   "name": "yolo_v8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
