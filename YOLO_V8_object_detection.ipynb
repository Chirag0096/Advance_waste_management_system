{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04969552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (8.0.205)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.22.2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (1.26.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (1.11.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.16.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.13.0)\n",
      "Requirement already satisfied: psutil in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
      "Requirement already satisfied: filelock in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.8.0)\n",
      "Requirement already satisfied: sympy in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: colorama in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f4226da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7eb7c89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import models\n",
    "# import torchvision.transforms as T\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "019d0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a644e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import cv2\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d31bd068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c54ac60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\reva_hack\\\\model'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce080a4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1015\u001b[0m                      map_location,\n\u001b[0;32m   1016\u001b[0m                      pickle_module,\n\u001b[0;32m   1017\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1018\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1427\u001b[0m )\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[0;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[1;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[1;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 274\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    276\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[1;32mD:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\torch\\serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[1;34m(location)\u001b[0m\n\u001b[0;32m    255\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    260\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    261\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    262\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    263\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "#model = torch.load('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbce8593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\reva_hack\\model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\reva_hack\\yolo_obj_detect\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "cd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dc4d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = \"D:\\reva_hack\\model\\best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fcf538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_=YOLO(\"best.pt\")\n",
    "#inference = torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb094279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ --------------------\n",
      "asttokens          2.4.1\n",
      "certifi            2023.7.22\n",
      "charset-normalizer 3.3.2\n",
      "colorama           0.4.6\n",
      "comm               0.1.4\n",
      "contourpy          1.1.1\n",
      "cycler             0.12.1\n",
      "debugpy            1.8.0\n",
      "decorator          5.1.1\n",
      "exceptiongroup     1.1.3\n",
      "executing          2.0.1\n",
      "filelock           3.13.1\n",
      "fonttools          4.43.1\n",
      "fsspec             2023.10.0\n",
      "gitdb              4.0.11\n",
      "GitPython          3.1.40\n",
      "idna               3.4\n",
      "ipykernel          6.26.0\n",
      "ipython            8.17.2\n",
      "jedi               0.19.1\n",
      "Jinja2             3.1.2\n",
      "jupyter_client     8.5.0\n",
      "jupyter_core       5.5.0\n",
      "kiwisolver         1.4.5\n",
      "MarkupSafe         2.1.3\n",
      "matplotlib         3.8.1\n",
      "matplotlib-inline  0.1.6\n",
      "mpmath             1.3.0\n",
      "nest-asyncio       1.5.8\n",
      "networkx           3.2.1\n",
      "numpy              1.26.1\n",
      "opencv-python      4.8.1.78\n",
      "packaging          23.2\n",
      "pandas             2.1.2\n",
      "parso              0.8.3\n",
      "Pillow             10.1.0\n",
      "pip                23.3.1\n",
      "platformdirs       3.11.0\n",
      "prompt-toolkit     3.0.39\n",
      "psutil             5.9.6\n",
      "pure-eval          0.2.2\n",
      "py-cpuinfo         9.0.0\n",
      "Pygments           2.16.1\n",
      "pyparsing          3.1.1\n",
      "pyserial           3.5\n",
      "python-dateutil    2.8.2\n",
      "pytz               2023.3.post1\n",
      "pywin32            306\n",
      "PyYAML             6.0.1\n",
      "pyzmq              25.1.1\n",
      "requests           2.31.0\n",
      "scipy              1.11.3\n",
      "seaborn            0.13.0\n",
      "setuptools         68.2.2\n",
      "six                1.16.0\n",
      "smmap              5.0.1\n",
      "stack-data         0.6.3\n",
      "sympy              1.12\n",
      "thop               0.1.1.post2209072238\n",
      "torch              2.1.0\n",
      "torchvision        0.16.0\n",
      "tornado            6.3.3\n",
      "tqdm               4.66.1\n",
      "traitlets          5.13.0\n",
      "typing_extensions  4.8.0\n",
      "tzdata             2023.3\n",
      "ultralytics        8.0.205\n",
      "urllib3            2.0.7\n",
      "wcwidth            0.2.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337c6233",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 292.5ms\n",
      "Speed: 5.2ms preprocess, 292.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 300.1ms\n",
      "Speed: 9.3ms preprocess, 300.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 313.6ms\n",
      "Speed: 0.0ms preprocess, 313.6ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 326.8ms\n",
      "Speed: 0.0ms preprocess, 326.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 293.9ms\n",
      "Speed: 0.0ms preprocess, 293.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 274.0ms\n",
      "Speed: 0.0ms preprocess, 274.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 2 drys, 1 plastic, 263.3ms\n",
      "Speed: 3.5ms preprocess, 263.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n",
      "dry\n",
      "plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 314.6ms\n",
      "Speed: 0.0ms preprocess, 314.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 281.4ms\n",
      "Speed: 0.0ms preprocess, 281.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 2 drys, 1 plastic, 280.0ms\n",
      "Speed: 0.0ms preprocess, 280.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic\n",
      "dry\n",
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 305.6ms\n",
      "Speed: 0.0ms preprocess, 305.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 305.1ms\n",
      "Speed: 0.0ms preprocess, 305.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 plastic, 286.9ms\n",
      "Speed: 0.0ms preprocess, 286.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 plastic, 314.8ms\n",
      "Speed: 0.0ms preprocess, 314.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 plastic, 318.2ms\n",
      "Speed: 0.0ms preprocess, 318.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 1 plastic, 281.5ms\n",
      "Speed: 0.0ms preprocess, 281.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic\n",
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 (no detections), 288.8ms\n",
      "Speed: 0.0ms preprocess, 288.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 273.4ms\n",
      "Speed: 0.0ms preprocess, 273.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 286.7ms\n",
      "Speed: 0.0ms preprocess, 286.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 dry, 305.5ms\n",
      "Speed: 0.0ms preprocess, 305.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dry\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 plastic, 308.0ms\n",
      "Speed: 0.0ms preprocess, 308.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "#from yolov8 import YOLOv8\n",
    "\n",
    "# Load the webcam stream\n",
    "cap = cv2.VideoCapture(0)\n",
    "detections_array = []\n",
    "# Define the inference callback\n",
    "def inference(frame):\n",
    "    # Convert the frame to RGB\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect objects in the frame\n",
    "    results = inference_.predict(frame, show=True)\n",
    "    names=inference_.names\n",
    "    import serial\n",
    "    import time\n",
    "\n",
    "    arduino = serial.Serial('COM4', 9600)  # Replace 'COMX' with your Arduino's serial port\n",
    "\n",
    "    def move_servo(direction):\n",
    "        arduino.write(direction.encode())\n",
    "        time.sleep(0.5)  # Adjust the delay to allow the servo to reach the desired position\n",
    "\n",
    "    # Your YOLOv8 object detection code here, which sets the `direction` variable based on detections\n",
    "    for r in results:\n",
    "        for c in r.boxes.cls:\n",
    "            detections_array.append(names[int(c)])\n",
    "            \n",
    "            print(names[int(c)])\n",
    "            if(names[int(c)]==\"dry\"):\n",
    "                direction='R'\n",
    "                move_servo('R')\n",
    "            else:\n",
    "                direction='L'\n",
    "                move_servo('L')\n",
    "                \n",
    "    # Example:\n",
    "#     detected_objects = [\"dry\", \"plastic\"]  # Replace with actual object names\n",
    "#     direction = 'R' if \"dry\" in detected_objects else 'L'  # Adjust the logic as needed\n",
    "\n",
    "    #move_servo(direction)\n",
    "    \n",
    "    #detections_array.append(results)\n",
    "    #print(results.names)\n",
    "#     \n",
    "\n",
    "#     for i in detections_array:\n",
    "#         if(i== \"dry\"):\n",
    "#             print(\"dry\")\n",
    "            \n",
    "#         if(i==\"plastic\"):\n",
    "#             print(\"plastic\")\n",
    "            \n",
    "#         if (i==\"wet\"):\n",
    "#             print(\"wet\")\n",
    "    # Draw bounding boxes around the detected objects\n",
    "#     for detection in results:\n",
    "#         cv2.rectangle(frame, (detection[0:2]), (detection[2:4]), (255, 0, 0), 2)\n",
    "#         cv2.putText(frame, detection[5], detection[0:2], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    \n",
    "#     if (labels==\"dry\"):\n",
    "#         print(\"dry\")\n",
    "#     if (labels==\"platic\"):\n",
    "#         print(\"plastic\")\n",
    "#     if(labels==\"wet\"):\n",
    "#         print(\"wet\")\n",
    "    # Return the frame\n",
    "    return frame\n",
    "\n",
    "# Start the inference loop\n",
    "while True:\n",
    "    # Get the next frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # If the frame is not empty, run the inference callback\n",
    "    if ret:\n",
    "        frame = inference(frame)\n",
    "\n",
    "    # Show the frame\n",
    "    #cv2.imshow(\"YOLOv8 Webcam Inference\", frame)\n",
    "\n",
    "    # Wait for a keypress\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Close the webcam\n",
    "cap.release()\n",
    "\n",
    "# Close all windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ca0aa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dry', 'plastic', 'wet', 'plastic', 'plastic', 'plastic', 'plastic', 'plastic', 'plastic', 'plastic', 'plastic', 'plastic', 'plastic', 'plastic', 'plastic', 'dry', 'plastic', 'plastic']\n"
     ]
    }
   ],
   "source": [
    "print(detections_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c890375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import time\n",
    "arduino = serial.Serial('COM4', 9600) \n",
    "def move_servo(direction):\n",
    "    arduino.write(direction.encode())\n",
    "    time.sleep(0.5)\n",
    "for i in detections_array:\n",
    "    if(i==\"dry\"):\n",
    "        move_servo('R')\n",
    "    if(i=='plastic'):\n",
    "        move_servo('L')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d0d459d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyserial\n",
      "  Downloading pyserial-3.5-py2.py3-none-any.whl (90 kB)\n",
      "     ---------------------------------------- 0.0/90.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 90.6/90.6 kB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: pyserial\n",
      "Successfully installed pyserial-3.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyserial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd3d2923",
   "metadata": {},
   "outputs": [],
   "source": [
    " def move_servo(direction):\n",
    "    arduino.write(direction.encode())\n",
    "    time.sleep(0.5)  # Adjust the delay to allow the servo to reach the desired position\n",
    "\n",
    "    # Your YOLOv8 object detection code here, which sets the `direction` variable based on detections\n",
    "    # Example:\n",
    "    detected_objects = [\"dry\", \"plastic\"]  # Replace with actual object names\n",
    "    direction = 'R' if \"dry\" in detected_objects else 'L'  # Adjust the logic as needed\n",
    "\n",
    "    move_servo(direction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d556039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x512 1 plastic, 308.1ms\n",
      "Speed: 5.5ms preprocess, 308.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 348.6ms\n",
      "Speed: 5.5ms preprocess, 348.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 278.5ms\n",
      "Speed: 4.0ms preprocess, 278.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 308.2ms\n",
      "Speed: 0.0ms preprocess, 308.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 288.0ms\n",
      "Speed: 0.0ms preprocess, 288.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 294.3ms\n",
      "Speed: 2.2ms preprocess, 294.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 305.6ms\n",
      "Speed: 0.0ms preprocess, 305.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 321.3ms\n",
      "Speed: 1.6ms preprocess, 321.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 323.2ms\n",
      "Speed: 1.5ms preprocess, 323.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 325.0ms\n",
      "Speed: 0.0ms preprocess, 325.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 285.4ms\n",
      "Speed: 5.7ms preprocess, 285.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 293.2ms\n",
      "Speed: 2.0ms preprocess, 293.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 303.5ms\n",
      "Speed: 4.0ms preprocess, 303.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 295.0ms\n",
      "Speed: 3.6ms preprocess, 295.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 315.4ms\n",
      "Speed: 2.5ms preprocess, 315.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 295.7ms\n",
      "Speed: 4.0ms preprocess, 295.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 2 plastics, 262.8ms\n",
      "Speed: 4.0ms preprocess, 262.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 311.5ms\n",
      "Speed: 2.2ms preprocess, 311.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 2 plastics, 305.0ms\n",
      "Speed: 0.6ms preprocess, 305.0ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 1 plastic, 295.1ms\n",
      "Speed: 2.5ms preprocess, 295.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 297.5ms\n",
      "Speed: 3.5ms preprocess, 297.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 dry, 2 plastics, 301.2ms\n",
      "Speed: 3.0ms preprocess, 301.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 272.1ms\n",
      "Speed: 3.0ms preprocess, 272.1ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 512)\n",
      "\n",
      "0: 384x512 1 plastic, 294.0ms\n",
      "Speed: 0.0ms preprocess, 294.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 512)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'detection_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 62\u001b[0m\n\u001b[0;32m     52\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# # Example: Simulated YOLOv8 detection results\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# detection_results = [\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#     {\"class\": \"object1\", \"confidence\": 0.85},\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Process the YOLOv8 detection results\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdetection_results\u001b[49m:\n\u001b[0;32m     63\u001b[0m     detected_class \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     64\u001b[0m     detected_classes\u001b[38;5;241m.\u001b[39mappend(detected_class)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'detection_results' is not defined"
     ]
    }
   ],
   "source": [
    "# import serial\n",
    "# import time\n",
    "\n",
    "# arduino = serial.Serial('COM4', 9600)  # Replace 'COMX' with your Arduino's serial port\n",
    "\n",
    "# # Initialize an empty list to store detected classes\n",
    "# detected_classes = []\n",
    "\n",
    "# def move_servo(direction):\n",
    "#     arduino.write(direction.encode())\n",
    "#     time.sleep(0.5)  # Adjust the delay to allow the servo to reach the desired position\n",
    "\n",
    "# # Your YOLOv8 object detection code here\n",
    "# import cv2\n",
    "\n",
    "# import numpy as np\n",
    "# #from yolov8 import YOLOv8\n",
    "\n",
    "# # Load the webcam stream\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# detections_array = []\n",
    "# # Define the inference callback\n",
    "# def inference(frame):\n",
    "#     # Convert the frame to RGB\n",
    "#     frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     # Detect objects in the frame\n",
    "#     results = inference_.predict(frame, show=True)\n",
    "#      # Return the frame\n",
    "#     return frame\n",
    "\n",
    "# # Start the inference loop\n",
    "# while True:\n",
    "#     # Get the next frame from the webcam\n",
    "#     ret, frame = cap.read()\n",
    "\n",
    "#     # If the frame is not empty, run the inference callback\n",
    "#     if ret:\n",
    "#         frame = inference(frame)\n",
    "\n",
    "#     # Show the frame\n",
    "#     #cv2.imshow(\"YOLOv8 Webcam Inference\", frame)\n",
    "\n",
    "#     # Wait for a keypress\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Close the webcam\n",
    "# cap.release()\n",
    "\n",
    "# # Close all windows\n",
    "# cv2.destroyAllWindows()\n",
    "   \n",
    "# # # Example: Simulated YOLOv8 detection results\n",
    "# # detection_results = [\n",
    "# #     {\"class\": \"object1\", \"confidence\": 0.85},\n",
    "# #     {\"class\": \"object2\", \"confidence\": 0.92},\n",
    "# #     {\"class\": \"object1\", \"confidence\": 0.78},\n",
    "# # ]\n",
    "\n",
    "# # Process the YOLOv8 detection results\n",
    "# for result in detection_results:\n",
    "#     detected_class = result[\"class\"]\n",
    "#     detected_classes.append(detected_class)\n",
    "#     direction = 'R' if detected_class == \"object1\" else 'L'  # Adjust the logic as needed\n",
    "\n",
    "#     # Move the servo based on the detected class\n",
    "#     move_servo(direction)\n",
    "\n",
    "#     # Print the detected classes\n",
    "#     print(f\"Detected Classes: {detected_classes}\")\n",
    "\n",
    "# # Close the serial connection when done\n",
    "# arduino.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "687e9fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(0)\n",
    "# while cap.isOpened():\n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     # Make detections \n",
    "#     results = inference_(frame)\n",
    "    \n",
    "#     #cv2.imshow('YOLO', np.squeeze(results.render()))\n",
    "    \n",
    "#     if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e63e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (8.0.205)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (3.8.1)\n",
      "Requirement already satisfied: numpy>=1.22.2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (1.26.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (1.11.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.1.0)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.16.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (2.1.2)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.13.0)\n",
      "Requirement already satisfied: psutil in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (5.9.6)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.7.22)\n",
      "Requirement already satisfied: filelock in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.8.0)\n",
      "Requirement already satisfied: sympy in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: colorama in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\reva_hack\\yolo_obj_detect\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b066c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ultralytics.yolo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv8\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetect\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpredict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DetectionPredictor\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ultralytics.yolo'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fe1b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_v8",
   "language": "python",
   "name": "yolo_v8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
